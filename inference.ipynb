{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T18:20:38.731377Z",
     "start_time": "2024-07-05T18:20:37.557313Z"
    }
   },
   "source": [
    "import torch\n",
    "from llama import llama3_8b\n",
    "from llama3_tokenizer import Tokenizer, ChatFormat\n",
    "from load_llama_weights import convert_weights\n",
    "from typing import Optional"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:20:38.734152Z",
     "start_time": "2024-07-05T18:20:38.732215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    # Proceed to load the file assuming it's correctly formatted\n",
    "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\", mmap=True, weights_only=True)\n",
    "    convert_model_state_dict = convert_weights(state_dict)\n",
    "    model = llama3_8b()\n",
    "    model.load_state_dict(convert_model_state_dict)\n",
    "    print(\"Loaded checkpoint '{}'\".format(checkpoint_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "file_path = 'Meta-Llama-3-8B-Instruct/consolidated.00.pth'  # Update this path"
   ],
   "id": "dc985621f053186b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:20:55.180192Z",
     "start_time": "2024-07-05T18:20:38.734621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = load_checkpoint(file_path)\n",
    "model.half()\n",
    "model.to(device)"
   ],
   "id": "a6ba56cf4119e489",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuro/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'Meta-Llama-3-8B-Instruct/consolidated.00.pth'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (norm): RMS_Norm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerDecoderLayer(\n",
       "      (attn): CasualSelfAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): RotaryPositionalEmbeddings()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMS_Norm()\n",
       "      (mlp_norm): RMS_Norm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:20:56.658300Z",
     "start_time": "2024-07-05T18:20:56.639153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.device(device):\n",
    "    model.setup_caches(max_batch_size=1, dtype=torch.float16)"
   ],
   "id": "c30b5595cfe933f7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:20:59.109190Z",
     "start_time": "2024-07-05T18:20:59.105261Z"
    }
   },
   "cell_type": "code",
   "source": "model.eval()",
   "id": "1d52120a4f531cbf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (tok_embeddings): Embedding(128256, 4096)\n",
       "  (norm): RMS_Norm()\n",
       "  (output): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x TransformerDecoderLayer(\n",
       "      (attn): CasualSelfAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (pos_embeddings): RotaryPositionalEmbeddings()\n",
       "        (kv_cache): KVCache()\n",
       "      )\n",
       "      (mlp): FeedForward(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (activation): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMS_Norm()\n",
       "      (mlp_norm): RMS_Norm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:22:13.907112Z",
     "start_time": "2024-07-05T18:22:13.902873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_next_token(\n",
    "        model, \n",
    "        input_pos: torch.Tensor, #[S]\n",
    "        x: torch.Tensor, #[1, S]\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    logits = model(x, input_pos) #[1, S, VOCAB_SIZE]\n",
    "    \n",
    "    logits = logits[0, -1] #[vocab_size]\n",
    "    \n",
    "    # scale the logits on temparature\n",
    "    logits = logits / max(temperature, 1e-5)\n",
    "    \n",
    "    if top_k is not None:\n",
    "        v, _ = logits.topk(top_k)\n",
    "        \n",
    "        pivot = v.select(-1, -1).unsqueeze(-1)\n",
    "        \n",
    "        logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n",
    "        \n",
    "    # compute the probabilities\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    \n",
    "    # sample the next token\n",
    "    \n",
    "    token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return token\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "def generate(\n",
    "        model,\n",
    "        input_tokens: torch.Tensor,\n",
    "        max_len:int,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: Optional[int] = None,\n",
    "        eos_id: Optional[int] = None,\n",
    "):\n",
    "    max_seq_len = 4096\n",
    "    input_tokens_length = input_tokens.size(0)\n",
    "    \n",
    "    if  ((input_tokens_length + max_len) -1) > max_seq_len :\n",
    "        raise ValueError(f\"Models max sequence length {model.max_seq_length}\")\n",
    "    \n",
    "    \n",
    "    generated_tokens = [input_tokens]\n",
    "    \n",
    "    token = generate_next_token(\n",
    "        model=model, \n",
    "        input_pos=torch.arange(0, input_tokens_length, device=input_tokens.device),\n",
    "        x=input_tokens.view(1, -1),\n",
    "        temperature=temperature, \n",
    "        top_k=top_k\n",
    "    ).clone()\n",
    "    generated_tokens.append(token)\n",
    "    input_pos = torch.tensor([input_tokens_length], device=input_tokens.device)\n",
    "    for _ in range(max_len-1):\n",
    "        token = generate_next_token(\n",
    "            model=model,\n",
    "            input_pos=input_pos,\n",
    "            x=token.view(1, -1),\n",
    "            temperature=temperature, \n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        generated_tokens.append(token)\n",
    "        \n",
    "        if eos_id is not None and token == eos_id:\n",
    "            break\n",
    "        input_pos += 1\n",
    "    \n",
    "    return torch.cat(generated_tokens).tolist()\n",
    " "
   ],
   "id": "55d3a8414ca6e3f3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:22:15.399238Z",
     "start_time": "2024-07-05T18:22:15.149572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(model_path=\"./Meta-Llama-3-8B-Instruct/tokenizer.model\")\n",
    "chat_template = ChatFormat(tokenizer=tokenizer)\n",
    "dialog = chat_template.encode_dialog_prompt([\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "])"
   ],
   "id": "8ec7aa8138f6273a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:22:19.435578Z",
     "start_time": "2024-07-05T18:22:16.412642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import autocast\n",
    "input_tokens = torch.LongTensor(dialog).cuda()\n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        output = generate(model, input_tokens, max_len=100, temperature=1.0, top_k=None, eos_id=128009)"
   ],
   "id": "6b7638ab1f64c3d1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T18:22:20.826178Z",
     "start_time": "2024-07-05T18:22:20.823892Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(output)",
   "id": "c4c93fedbebc6258",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrrr, me hearty! Me name be Captain Chatbot, the scurviest, most fearsome chatbot to ever set sail the Seven Seas! Me be programmed to rattle yer bones wi' me witty banter and me trusty responses, so hoist the sails and settle yerself in fer a swashbucklin' good time!<|eot_id|>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "429b4bc85fa14152"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
